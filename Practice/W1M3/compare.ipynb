{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "# 로그 기록 시작 함수\n",
    "def log_started():\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "        log_file.write(f\"New execution at {timestamp}\")\n",
    "        log_file.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 로그 기록 함수\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%B-%d-%H-%M-%S')\n",
    "    with open('etl_project_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{timestamp} - {level} - {message}\\n\")\n",
    "\n",
    "# 설정 파일 읽기\n",
    "def load_config(config_path='config.ini'):\n",
    "    if not os.path.exists(config_path):\n",
    "        log_message(f\"Configuration file '{config_path}' not found.\", level=\"ERROR\")\n",
    "        raise FileNotFoundError((f\"Configuration file '{config_path}' not found.\"))\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    if 'DEFAULT' not in config or 'URL' not in config['DEFAULT'] or 'TABLE_CLASS' not in config['DEFAULT']:\n",
    "        log_message(\"Invalid or missing configuration values in 'config.ini'.\", level=\"ERROR\")\n",
    "        raise ValueError(\"Invalid or missing configuration values in 'config.ini'.\")\n",
    "    \n",
    "    return config['DEFAULT']['URL'], config['DEFAULT']['TABLE_CLASS']\n",
    "    \n",
    "\n",
    "# Save\n",
    "def save_gdp_data(df, output_csv_file='extracted_gdp_data.csv', output_json_file='extracted_gdp_data.json'):\n",
    "    log_message(\"Saving Extracted Data\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        df.to_json(output_json_file, orient='records', force_ascii=False, indent=4)\n",
    "        log_message(f\"Data saved: CSV ({output_csv_file}), JSON ({output_json_file})\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Failed to save data: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "def extract_gdp_data(url, table_class):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status() # HTTP 응답 상태 코드를 확인. 200번대가 아닌 경우(예: 404, 500 등) HTTPError 예외를 발생시킴\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': table_class})\n",
    "        \n",
    "        if table is None:\n",
    "            log_message(\"No table found with the specified class.\", level=\"ERROR\")\n",
    "            raise ValueError(\"Failed to locate the GDP table on the webpage.\")\n",
    "            \n",
    "        df = pd.read_html(str(table))[0]  # 위키피디아에서 제공하는 표를 Pandas로 읽고 객체를 문자열로 변환\n",
    "        \n",
    "        df = df.iloc[:, [0, 1, 2]]  # 필요한 칼럼만 선택 (모든 행과 0, 1, 2번째 열을 선택)\n",
    "        df.columns = ['Country', 'GDP (Nominal)', 'Year']\n",
    "        \n",
    "        df = df.dropna(subset=['Country', 'GDP (Nominal)']) # NaN 데이터 제거 \n",
    "        df['GDP (B USD)'] = ( # GDP 값 정리 및 변환\n",
    "            df['GDP (Nominal)']\n",
    "            .str.replace(r'[^\\d.]', '', regex=True)  # 숫자와 소수점 이외 제거\n",
    "            .replace('', '0')  # 빈 문자열을 '0'으로 대체\n",
    "            .astype(float)  # float으로 변환\n",
    "            / 1e3  # 단위를 B USD로 변환\n",
    "        )\n",
    "        df['Year'] = df['Year'].str.replace(r'\\[.*?\\]', '', regex=True) # 각주 제거 (sup 이런 게 자꾸 따라와서..)\n",
    "        df = df[['Country', 'GDP (B USD)', 'Year']]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data extraction: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform_gdp_data(df):\n",
    "    log_message(\"Starting Data Transmission\")\n",
    "    try:\n",
    "        log_message(\"Starting Data Transformation in parallel\")\n",
    "\n",
    "        # Region 데이터를 미리 로드\n",
    "        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:\n",
    "            region_data = json.load(region_file)\n",
    "\n",
    "        def transform_xchunk(chunk):\n",
    "            # GDP 정렬 및 반올림\n",
    "            chunk = chunk.sort_values(by='GDP (B USD)', ascending=False)\n",
    "            chunk['GDP (B USD)'] = chunk['GDP (B USD)'].round(2)\n",
    "            \n",
    "            # Region 데이터를 연결\n",
    "            chunk['Region'] = chunk['Country'].map(region_data)\n",
    "            return chunk\n",
    "\n",
    "        # 데이터프레임 분할 및 병렬 처리\n",
    "        num_partitions = 5\n",
    "        chunks = np.array_split(df, num_partitions)\n",
    "        transformed_chunks = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(transform_chunk, chunk) for chunk in chunks]\n",
    "            for future in as_completed(futures):\n",
    "                transformed_chunks.append(future.result())\n",
    "        \n",
    "        # 결과 병합\n",
    "        transformed_data = pd.concat(transformed_chunks)\n",
    "        return transformed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during data transformation: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def load_gdp_data(df):\n",
    "    log_message(\"Loading data into SQLite database\")\n",
    "    try:\n",
    "        # SQLite 데이터베이스에 연결\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        \n",
    "        df[['Country', 'GDP (B USD)', 'Year', 'Region']].rename(\n",
    "            columns={'GDP (B USD)': 'GDP_USD_billion'}\n",
    "        ).to_sql( # 데이터프레임 데이터를 SQL 테이블로 변환하여 데이터베이스에 저장하는 pandas 메서드입\n",
    "            'Countries_by_GDP', conn, if_exists='replace', index=False\n",
    "        )\n",
    "        \n",
    "        conn.close()\n",
    "        log_message(\"Data successfully loaded into SQLite database\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error while loading data into SQLite database: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# GDP가 100B USD 이상인 국가 필터링\n",
    "def filtered_100USD(df):\n",
    "    filtered_100 = df[df['GDP (B USD)'] >= 100]\n",
    "    print(\"Countries with a GDP of over 100B USD\")\n",
    "    print(filtered_100)\n",
    "    return filtered_100\n",
    "\n",
    "\n",
    "# Region별 상위 5개 국가의 GDP 평균 계산\n",
    "def region_top5_calculate(df):\n",
    "    region_top5_avg = (\n",
    "        df.groupby('Region')\n",
    "        .apply(lambda x: x.nlargest(5, 'GDP (B USD)')['GDP (B USD)'].mean())\n",
    "        .reset_index(name='Top 5 Avg GDP (B USD)')\n",
    "    )\n",
    "    print(\"Average GDP of top 5 countries by region\")\n",
    "    print(region_top5_avg)\n",
    "    return region_top5_avg\n",
    "\n",
    "\n",
    "# 추가 요구사항 구현\n",
    "def display_countries_with_gdp_over_100():\n",
    "    log_message(\"Displaying countries with GDP over 100B USD\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"SELECT Country, GDP_USD_billion FROM Countries_by_GDP WHERE GDP_USD_billion >= 100\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Countries with GDP >= 100B USD:\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for countries with GDP >= 100B: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "    \n",
    "# Region별 상위 5개 국가의 GDP 평균 계산 및 출력\n",
    "def display_region_top5_average_gdp():\n",
    "    log_message(\"Calculating average GDP of top 5 countries by region\")\n",
    "    try:\n",
    "        conn = sqlite3.connect('World_Economies.db')\n",
    "        query = \"\"\"\n",
    "        WITH RankedCountries AS (\n",
    "            SELECT Country, GDP_USD_billion, Region,\n",
    "                   RANK() OVER (PARTITION BY Region ORDER BY GDP_USD_billion DESC) AS Rank\n",
    "            FROM Countries_by_GDP\n",
    "            WHERE Region IS NOT NULL\n",
    "        )\n",
    "        SELECT Region, AVG(GDP_USD_billion) AS Avg_Top5_GDP\n",
    "        FROM RankedCountries\n",
    "        WHERE Rank <= 5\n",
    "        GROUP BY Region\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Average GDP of top 5 countries by region (excluding None):\")\n",
    "        print(tabulate(result, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error querying database for top 5 average GDP: {str(e)}\", level=\"ERROR\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def etl_process():\n",
    "    try:\n",
    "        # 시작 시간 기록\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        log_started()\n",
    "        log_message(\"ETL Process Started\")\n",
    "        \n",
    "        # 설정 로드\n",
    "        url, table_class = load_config()\n",
    "        \n",
    "        # Extract\n",
    "        extracted_data = extract_gdp_data(url, table_class)\n",
    "        \n",
    "        # Save Extracted Data\n",
    "        save_gdp_data(extracted_data)\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = transform_gdp_data(extracted_data)\n",
    "        \n",
    "        # Save Transformed Data\n",
    "        save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')\n",
    "        \n",
    "        # Load into SQLite Database\n",
    "        load_gdp_data(transformed_data)\n",
    "\n",
    "        # Additional Analyses\n",
    "        display_region_top5_average_gdp()\n",
    "        display_countries_with_gdp_over_100()\n",
    "\n",
    "\n",
    "        log_message(\"ETL Process Completed Successfully\")\n",
    "        \n",
    "        # 종료 시간 기록 및 소요 시간 계산\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # 소요 시간 로그에 기록 및 출력\n",
    "        log_message(f\"ETL Process Duration: {elapsed_time}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"ETL Process Failed: {str(e)}\", level=\"ERROR\")\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
