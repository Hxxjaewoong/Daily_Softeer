# 📅 2025.01.17

---

## 오늘의 Todo List
- [✔️] W1M3 리팩토링 
    - [✔️] 내 코드를 모르는 사람이 처음 보았을 때, 이해 가능하도록 하는 설명 및 주석 추가하기
    - [✔️] 로그 및 주석 작성 시 grammar가 정확한지 확인하기
    - [✔️] E, T를 나눌 때 데이터 특성을 고려하여 나누기
    - [✔️] Region 정보를 붙일 때의 병렬처리에 대한 문제 고치기
    - [✔️] 데이터 저장하는 부분을 Transform 안으로 옮기기
    - [✔️] **try문에서 오류를 더 정확히 확인하기 위해서 더 작게 쪼갤 필요가 있음**
- [✔️] 팀원과 리팩토링된 W1M3 의견 공유
- [✔️] Docker file 한 줄씩 실행해보기   


---

# 리뷰 & 회고

---


## 📝 리뷰

> 리팩토링한 W1M3 팀원들과 의견 공유하기 (내 코드에 대한 팀원들의 생각)
- transform 단계에서 크게 보면 3단계로 나눠서 변환 작업을 수행하는데, 중간에 오류가 발생했을 때 그 전까지의 작업물까지는 저장해야할 필요가 있지 않을까? ==> 내 생각: 지금과 다르게 대용량 데이터를 다루게 된다면, 파일은 읽고 쓰는 것 자체가 매우 큰 비용이 들텐데, 작은 작업 하나하나 마다 cpu bound인 transform 단계에서 읽고 쓰기를 반복한다면, 너무 비용이 크지 않을까? 라는 생각이다. 파일을 읽는 작업이 아니라 파일을 저장하는 작업까지는 해도 괜찮을지도 모르겠다. 추가적인 고민이 필요할 듯 하다.
- extract 나 transform 단계에서 지금과 다르게 정말 방대한 데이타를 다뤄야한다면, 지금과는 다르게 chunk 단위로 나눠서 처리하는 방법을 고민해보았는가? ==> 그것에 대해 test 파일로 100만개의 인덱스를 가진 데이터를 만들어서 청크를 나눈 후 ThreadPoolExecutor() 를 이용하여 실험을 해봤는데, 유의미한 결과를 얻지는 못했다. 해당 방법이 아닌, 이번주에 실습을 진행했던 multiprocessing을 이용해서 처리하는 방법에 대해 고민해보고 실험해볼 예정

> 코드 리팩토링을 하며 느낀 점 & 고친 부분


>> 코딩 스타일 가이드를 생각보다 잘 지키진 않은 것 같았다.
- PEP 8 – Style Guide for Python Code
- 개인적으로 이것만은 꼭 지켜야한다고 다짐
    - 블럭 주석: 코드가 수행하는 작업을 설명하며, #으로 시작
    - 인라인 주석: 코드와 같은 줄에 작성하며, 코드와 주석 사이에는 최소 두 칸의 공백
    - Docstrings: 함수, 클래스, 모듈의 목적을 설명하며, 3개의 따옴표(""")로 
    - 명시적인 예외 처리를 사용하고, 구체적인 예외를 명시할 것



>> try문에서 오류를 더 정확히 확인하기 위해서 더 작게 쪼갤 필요가 있음
- try 블록이 크면 예외가 어디에서 발생했는지 알기 어려우므로, 블록을 작게 나누어 예외가 발생한 코드 라인을 정확히 파악한 뒤 디버깅 속도를 빠르게 높일 수 있을 것 같다.
- 코드의 각 부분에서 발생할 수 있는 예외는 다를 수 있으니 예외 처리 로직을 구체적으로 작성할 수 있을 것이다.


>> 코드 수정 요약
- 내 코드를 처음 보는 사람이 보았을 때 이해하기 쉽도록 로직 변경 및 동작에 대한 주석 추가  
```python
    """
    Extract GDP data from a webpage based on a configuration file.

    Parameters:
        config_path (str): Path to the configuration file (default: 'config.ini').

    Returns:
        pd.DataFrame: Raw GDP data extracted from the webpage.
    """
```
- load_config() 함수에서 실행하던 설정 파일을 읽어오는 동작을 extract로 통합
```python
def extract_gdp_data(config_path='config.ini'):
    """
    Extract GDP data from a webpage based on a configuration file.

    Parameters:
        config_path (str): Path to the configuration file (default: 'config.ini').

    Returns:
        pd.DataFrame: Raw GDP data extracted from the webpage.
    """
    log_message("Starting data extraction process")

    # Step 1: Read and validate the configuration file
    if not os.path.exists(config_path):
        log_message(f"Configuration file '{config_path}' not found.", level="ERROR")
        raise FileNotFoundError(f"Configuration file '{config_path}' not found.")

    try:
        config = configparser.ConfigParser()
        config.read(config_path)
        url = config['DEFAULT']['URL']
        table_class = config['DEFAULT']['TABLE_CLASS']
        log_message("Configuration file read successfully")
    except Exception as e:
        log_message(f"Error reading configuration file: {str(e)}", level="ERROR")
        raise
```
- extract에서 진행하던 일부작업을 모두 transform으로 옮기고, extract에서는 raw data만 저장할 수 있도록 변경
```python
    try:
        raw_df = pd.read_html(str(table))[0]
        raw_df.to_csv('raw_gdp_data.csv', index=False)
        raw_df.to_json('raw_gdp_data.json', orient='records', force_ascii=False, indent=4)
        log_message("Raw data saved successfully")
    except Exception as e:
        log_message(f"Error converting table to DataFrame or saving raw data: {str(e)}", level="ERROR")
        raise

    log_message("Data extraction process completed successfully")
    return raw_df
```

- Python의 반복문은 Pandas의 병합 연산보다 속도가 느리므로 transform에 해당 부분 개선
1. Python 반복문 vs Pandas 병합

	- Python 반복문
		-	반복문 방식(map 또는 apply)은 각 행마다 순차적으로 데이터를 처리
		-	각 국가에 대해 JSON 데이터를 검색하고 매칭해야 하므로 ￼의 시간 복잡도가 발생
		-	Python의 반복문은 인터프리터 방식으로 실행되기 때문에, 대량 데이터셋에서 비효율적

	- Pandas 병합
		-	Pandas 병합은 벡터화 연산을 사용하며, 내부적으로 최적화된 C 라이브러리를 통해 동작
		-	대량 데이터셋에서도 병합 속도가 훨씬 빠름

2. JSON 데이터를 Pandas DataFrame으로 변환

	- 변경된 코드에서 json.load()로 읽은 데이터를 Pandas DataFrame으로 변환했기 때문에:
		1. 	JSON 데이터를 DataFrame으로 한 번만 변환하면 됨
		2.	Pandas 병합 연산으로 모든 국가와 지역 정보를 한 번에 매칭할 수 있다.

3. 연산 속도의 차이
	- 기존 방식 (Python 반복문):
		- 각 행마다 JSON 딕셔너리를 검색하며, 반복적으로 키를 확인해야 하므로 시간이 오래 걸린다.
	- 변경된 방식 (Pandas 병합):
		- 두 DataFrame을 효율적으로 병합하며, 전체 데이터를 벡터화 연산으로 처리하기 때문에 실행 속도가 매우 빨라짐.

- 변경전
```python
with open('country_region_table.json', 'r', encoding='utf-8') as region_file:
            region_data = json.load(region_file)
        df['Region'] = df['Country'].map(region_data)
        log_message("Data Transformation Completed")
```

- 변경 후
```python
        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:
            region_data = json.load(region_file)
        region_df = pd.DataFrame(list(region_data.items()), columns=['Country', 'Region'])
        df = pd.merge(df, region_df, on='Country', how='left')
```

- transform_gdp_data(df)에서 진행하는 과정 중 try문에서 오류를 더 정확히 확인하기 위해서 더 작게 쪼갬
```python
def transform_gdp_data(df):
    """
    Transform the extracted GDP data for further analysis.

    Parameters:
        df (pd.DataFrame): Raw GDP data.

    Returns:
        pd.DataFrame: Transformed GDP data.
    """
    log_message("Starting data transformation process")
    try:
        # Step 1: Clean and process the data
        df = df.iloc[:, [0, 1, 2]]
        df.columns = ['Country', 'GDP (Nominal)', 'Year']
        df = df.dropna(subset=['Country', 'GDP (Nominal)'])
        df['GDP (B USD)'] = (
            df['GDP (Nominal)']
            .str.replace(r'[^\d.]', '', regex=True)  # Remove non-numeric characters
            .astype(float) / 1e3
        )
        df['Year'] = df['Year'].str.replace(r'\[.*?\]', '', regex=True)
        df = df[['Country', 'GDP (B USD)', 'Year']]
        df.sort_values(by='GDP (B USD)', ascending=False, inplace=True)
        log_message("Data cleaned and transformed successfully")

        # Step 2: Merge with region data
        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:
            region_data = json.load(region_file)
        region_df = pd.DataFrame(list(region_data.items()), columns=['Country', 'Region'])
        df = pd.merge(df, region_df, on='Country', how='left')

        # Step 3: Save transformed data
        df.to_csv('transformed_gdp_data.csv', index=False)
        df.to_json('transformed_gdp_data.json', orient='records', force_ascii=False, indent=4)
        log_message("Transformed data saved successfully")

    except Exception as e:
        log_message(f"Error in data transformation: {str(e)}", level="ERROR")
        raise

    return df
```

> 멀티프로세싱 추가 공부

> Multiprocessing에서 Queue를 사용하는 이유?
- Queue는 멀티프로세싱 환경에서 프로세스 간 통신을 위해 사용 (멀티프로세싱 환경에서는 각 프로세스가 독립적인 메모리 공간을 가지므로, 직접적인 데이터 공유가 어렵다)
- Queue는 내부적으로 락을 사용하여 동기화를 처리하므로, 여러 프로세스가 동시에 접근해도 데이터의 일관성을 유지할 수 있다
- 락을 사용하면 여러 프로세스가 동시에 큐에 접근하더라도 데이터의 일관성을 유지가능
- queue.put(item)과 queue.get()은 락을 사용하여 안전하게 데이터를 추가하고 제거

> Queue의 동기화 
- 락 생성
    - Queue 객체가 생성될 때, 내부적으로 락 객체도 함께 생성
    - 이 락 객체는 큐에 대한 접근을 제어하는 데 사용됨

- 데이터 추가 (put)
    - 프로세스가 queue.put(item)을 호출할 때, 먼저 락을 획득
    - 락을 획득한 프로세스만 큐에 데이터를 추가할 수 있음
    - 데이터 추가가 완료되면 락을 해제
    - 다른 프로세스는 락이 해제될 때까지 대기

- 데이터 제거 (get)
    - 프로세스가 queue.get()을 호출할 때, 먼저 락을 획득
    -  락을 획득한 프로세스만 큐에서 데이터를 제거할 수 있음
    - 데이터 제거가 완료되면 락을 해제
    - 다른 프로세스는 락이 해제될 때까지 대기

    
> Lock?
- **락(lock)**은 멀티스레딩이나 멀티프로세싱 환경에서 공유 자원에 대한 접근을 제어하기 위해 사용
- 락을 사용하면 여러 스레드나 프로세스가 동시에 자원에 접근하는 것을 방지하여 데이터의 일관성과 무결성을 유지 가능
파이썬에서는 threading.Lock과 multiprocessing.Lock을 사용하여 락을 구현 가능
- 락의 동작 원리
    - 락 획득 (Acquire): 스레드나 프로세스가 공유 자원에 접근하기 전에 락을 획득. 락이 이미 다른 스레드나 프로세스에 의해 획득된 상태라면, 현재 스레드나 프로세스는 락이 해제될 때까지 대기
    - 자원 접근 (Access Resource): 락을 획득한 스레드나 프로세스만이 공유 자원에 접근 가능. 이 동안 다른 스레드나 프로세스는 자원에 접근할 수 없음
- 락 해제 (Release): 공유 자원에 대한 작업이 완료되면 락을 해제. 락이 해제되면 대기 중인 다른 스레드나 프로세스 중 하나가 락을 획득 가능

> Use Queue.get_nowait() and exception handling to manage tasks dynamically between processes.
1. Queue.get_nowait()를 왜 사용할까?
    - 작업이 더 이상 없을 때 프로세스가 대기 상태에 빠지지 않도록 ==> 여러 프로세스가 동적으로 작업을 분배받아 수행할 때 유용함
2. 예외처리에 관하여
    - get_nowait()는 큐가 비어 있는 경우 queue.Empty 예외를 발생
    - 예외를 처리하지 않으면 프로그램이 충돌할 수 있으므로, try-except 블럭을 사용하자
3. 1번에 동적으로 작업을 관리한다는 의미가 어떤 의미일까?
    - 하나의 큐에 타스크(task)를 넣고 여러 프로세스가 이를 동적으로 가져가서 처리
    - 작업이 남아있는 동안만 가져가고, 없으면 대기하지 않고 종료함
    - 프로세스가 큐에서 작업을 가져올 때, 작업이 없으면 queue.Empty 예외를 처리하여 정상적으로 종료할 수 있도록 


> 코드 비교
```python
while not tasks_that_are_done.empty():
    print(tasks_that_are_done.get())
```
- empty()는 큐가 변경되지 않을 때는 정확한 결과를 반환하겠지만
- 동시성 환경에서는 레이스 컨디션이 발생할 듯하다 (다른 프로세스가 empty() 호출 후 큐에 항목을 추가하거나 제거하면, empty()의 결과와 실제 상태가 달라질 듯) ==> 다노님 ppt의 non-atomic이 해당 부분과 연결되는 듯
- 상태 확인 후 작업을 가져옴 (비원자적) ==> 큐 접근 두번


```python
while True:
    try:
        print(tasks_that_are_done.get_nowait())
    except queue.Empty:
        break
```
- 레이스 컨디션 문제를 방지
- 작업이 없을 때 자연스럽게 예외로 종료하므로, 동적 환경에서 안전하게 동작한다
- 예외 처리를 통해 작업이 없는 상황에서의 종료를 명확하게 표현
- 상태 확인과 작업 가져오기를 한 번에 수행 (원자적) ==> 큐 접근 한 번


> 멀티프로세싱 환경에서 Python은 GIL이 없으나, OS 스케줄러에 의해 프로세스의 실행 우선 순위가 달라질 수 있다?
1. Python의 GIL(Global Interpreter Lock)
    - Python 인터프리터가 한 번에 하나의 스레드만 Python 바이트코드를 실행하도록 제한하는 것
    - 멀티스레드 환경에서 데이터 무결성을 보장하기 위해 설계됨
2. 멀티 프로세싱
    - 멀티 프로세싱은 GIL의 제약을 우회. 
    - 각 프로세스는 독립적인 python 인터프리터와 메모리 공간을 가지므로 영향을 받지 않음
    - 스레드: 같은 메모리 공간을 공유하며, GIL에 의해 제한됨
	- 프로세스: 독립적인 메모리 공간과 인터프리터를 가지므로 GIL의 제한을 받지 않음
3. OS 스케줄러란?
    - 운영체제는 CPU의 사용을 최적화하기 위해 스케줄러를 사용하여 프로세스와 스레드의 실행 순서를 결정함
	- 스케줄러는 우선 순위(priority), CPU 사용량, I/O 대기 상태 등의 요인을 기반으로 실행 순서를 조정
	- 멀티프로세싱과 OS 스케줄러:
	    - Python 멀티프로세싱에서 여러 프로세스가 생성되면, 각 프로세스는 독립적인 실행 단위로 OS 스케줄러에 의해 관리됨
	    - 스케줄러는 각 프로세스가 사용 가능한 CPU 코어에 할당되도록 조정하며, 실행 우선 순위를 다르게 적용할 수도 있음 (나의 M4의 출력 결과 이유)
	    - 실행 우선 순위는 프로세스 생성 시점, 작업의 성격(I/O vs CPU), 그리고 시스템 부하에 따라 달라질 수 있음


---

## 🔍 회고 (KPT)

### ✅ Keep
- 백엔드 강사님(?)께서 오셔서 구경하시길래 간단하게 내 코드의 문제점을 여쭤봄 ==> try문에서 오류를 더 정확히 확인하기 위해서 더 작게 쪼개라고 조언해주심. 어제는 comfort zone에만 머물렀던 것 같은데 무언가 작지만 용기를 냈더니 조금이라도 성장할 수 있었음 !!
- 일과를 시작하기 전, 오늘 하루 해야할 일을 평소보다 훨씬 specific하게 정리하고 시작했던 것 ==> 나중에 할 일이 뭐였었는지 기억할 필요 없이 바로바로 빠르게 다음 작업이 가능했다.


### ⚠️ Problem
- 실습했던 것들을 하나씩만 적용하고 모두 적용하진 못했던 것 ==> ETL 코드를 리팩토링 할 때, 이번주에 배웠던 multiprocessing도 적용해봤으면 어땠을까? ==> 결과가 어떻든 일단 시도해봤어야함

### 💡 Try
- 팀원 산야님 코드를 보고, 나도 class를 적극적으로 더 활용해봐야겠다고 생각함

---
