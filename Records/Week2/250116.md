# 📅 2025.01.15


---


> M5 추가 요구 사항 - word cloud의 작동 방법 (어떤 로직으로 시각화가 이루어지는지)

>> Word Cloud란?
- 태그 클라우드라고도 불리우며, 문서의 키워드, 개념 등을 직관적으로 파악할 수 있도록 핵심 단어를 시각화하는 기법
- 빅데이터를 분석할 때 데이터의 특징을 도출하거나 해당 데이터의 주요 키워드를 알아볼 때 많이 사용함

>> 텍스트 데이터 전처리 과정
- 자연어 처리 (NLP) 기법을 사용하여 텍스트를 단어 또는 문장 단위로 토큰화함. 정규 표현식이나 spaCy, NLTK 같은 라이브러리가 사용되는 듯 하다.
- 정규 표현식을 활용하거나 전처리 라이브러리를 사용해서 텍스트 정규화

>> 단어 빈도 계산
- **Term Frequency (TF) 사용하여** 각 단어의 등장 횟수를 계산. 이는 Word Cloud에서 단어의 크기와 직접적으로 연관.
- **TF-IDF (Term Frequency-Inverse Document Frequency)를** 단어의 중요도를 강조하기 위해 사용.  단어가 특정 문서에서 자주 등장하지만, 전체 문서에서는 드물게 등장할 때 중요도가 높아지는 기법

>> 단어 배치 알고리즘
Word Cloud는 효율적인 공간 활용과 시각적 매력을 위해 다양한 배치 알고리즘을 사용함
-  Filling by Rectangle Packing
    - 단어는 직사각형 박스로 간주되며, 가장 큰 단어를 중심부터 배치
    - 각 단어를 배치할 때 충돌을 방지하고, 기존 단어와 겹치지 않도록 최적의 위치를 계산
    - 이 과정은 `Bounding Box Collision Detection` 알고리즘으로 구현

- Spiral Layout Algorithm
    - 단어는 중심에서 나선형으로 배치되며, 충돌이 발생하지 않을 때까지 이동

- Shape Constraining
    - 특정 모양(예: 원형, 사각형, 사용자 정의 이미지)에 단어를 배치
    - 이를 위해 이미지의 픽셀 밀도를 기준으로 단어가 배치 가능한 영역을 정의

>> 최적화 기법
- Greedy Algorithm: 공간을 효율적으로 활용하기 위해 단어 배치를 최적화
- Heuristic Search: 배치 충돌을 최소화하는 최적의 단어 위치를 탐색
- Parallel Computing: 큰 데이터셋의 경우 병렬 처리를 통해 성능을 향상


> M5 summary를 읽은 후, 나의 소감  

데이터를 통하여 선거 결과를 예측하는 방법과 그 결과가 매우 혁신적이었다고 느꼈다. 또한 전통적인 방법과 비교했을 때보다 소셜 미디어 데이터를 활용한 방법이 더 높은 정확성을 갖는다는 사실이 놀라웠다. 전통적인 방법이라면 사전 투표 조사나 전화 조사 등과 같은 꽤나 직접적인 방법을사용했을텐데, 비교적 속마음이 더 많이 표현되어서일까? 정확도가 더 높았다는 사실이 놀라웠다. 

개인적으로, 단순히 데이터 과학 기술의 성공적인 활용 사례를 넘어, 기술과 사회적 문제의 연결 가능성을 보여주는 좋은 예라고 느꼈다. 이러한 데이터를 이용한 방식이 전통적인 방법보다 더 빠르고 정확한 결과를 낼 뿐만 아니라, 대중의 목소리를 더욱 직접적으로 반영한다는 점에서 의미가 있는 것 같다. 특히 트위터라는 플랫폼이 가진 실시간성과 방대한 사용자 기반을 활용하여 94.2%의 예측 정확도를 달성한 것은 매우 놀라웠다.  

이를 통해 느낀 또 다른 중요한 점은, 소셜 미디어 데이터가 선거 결과 예측뿐만 아니라, 사회적 문제 해결에 광범위하게 활용될 수 있는 잠재력을 가지고 있다는 것아다. 예를 들어, 정책의 효과를 실시간으로 평가하거나, 대중의 요구를 더 잘 반영한 의사결정을 내리는 데에도 큰 역할을 할 수 있을 것 같다.  

물론 아러한 접근법이 가지는 현실적인 한계도 생각해보게 되었다. 트위터 데이터가 모든 유권자 집단을 대표하지 못할 가능성, 또는 허위 정보와 자동화된 봇 계정의 영향 같은 문제가 여전히 존재하지 않을까 싶다. 하지만 이러한 한계를 극복하기 위한 추가 연구가 이루어진다면, 지금의 높은 정확도보다 더 높은 신뢰도와 적용 가능성을 확보할 수 있을 것 같다. 
  
결론적으로 생각해보면 논문에서 사용된 방법론과 결과는 데이터 과학이 정치 및 사회적 현상 분석에 실질적으로 기여할 수 있음을 잘 보여준 것이 아닐까 싶다.

 


---

# 리뷰 & 회고


---


## 📝 리뷰

> 내 코드 작성 방법에 대한 리뷰
- Dano 님의 말씀을 듣고, W1M1부터 지금까지 진행했던 모든 미션 안에 작성된 나의 코드를 다시 보았다.
- **내 코드를 모르는 사람이 처음 보았을 때, 이해 가능하도록 하는 설명 및 주석이 부족했다**
- W1M3의 수정을 우선 순위로 하여, 첫 미션부터 지금까지 진행한 미션들에게 모두 '내가 아닌, 다른 사람, 그리고 타겟을 대상'으로 작성된 코드가 되도록 수정을 진행해야 한다.

> ETL에 대한 리뷰
- W1M3를 진행할 때, 동작을 먼저 우리의 언어 (pseudo code)로 먼저 작성 후 코드를 작성하지 못한 것이 아쉽다. ==> 미리 구성을 나눠 작성해놓을 것
- 또한 로그 및 주석 작성 시 grammar가 정확했나? 모르는 사람이 봤을 때 이해하기 쉬웠나 ==> 생각보다 부족한 점이 많았음 
    - 함수 전에 주석으로 동작 설명 및 input output에 대한 정의를 추가해야했다
- E, T를 나눌 때 데이터 특성을 고려하지 못하고 코드를 작성한 것에 대한 문제점 존재 ==> E에서 하지 않아도 되는 불필요한 과정이 존재한다. 이는 T에서 실행하도록 구성했어야함. 
- E, T의 데이터 특성을 고려해서 E (i/o bound) 목적에 맞게 코드를 구성할 것
```python
### 내 코드 문제점 부분 ###
# 필요한 칼럼만 선택 및 이름 변경
        df = df.iloc[:, [0, 1, 2]]  # 첫 번째, 두 번째, 세 번째 칼럼만 선택
        df.columns = ['Country', 'GDP (Nominal)', 'Year']  # 칼럼 이름 설정
        # NaN 데이터 제거
        df = df.dropna(subset=['Country', 'GDP (Nominal)'])
        # GDP 값 정리 및 변환
        df['GDP (B USD)'] = (
            df['GDP (Nominal)']
            .str.replace(r'[^\d.]', '', regex=True)  # 숫자와 소수점 이외 제거
            .replace('', '0')  # 빈 문자열을 '0'으로 대체
            .astype(float)  # float으로 변환
            / 1e3  # 단위를 B USD로 변환
        )
        # 각주 제거
        df['Year'] = df['Year'].str.replace(r'\[.*?\]', '', regex=True)
        # 필요한 3개 칼럼만 유지
        df = df[['Country', 'GDP (B USD)', 'Year']]
        # GDP 내림차순 정렬
        df = df.sort_values(by='GDP (B USD)', ascending=False)
        # 소수점 2자리로 반올림
        df['GDP (B USD)'] = df['GDP (B USD)'].round(2)
```
- Region 정보를 붙일 때의 병렬처리에 대한 문제점 ==> 판다스의 장점을 활용하지 않고 데이터를 데이터 프레임에서 하나씩 일일히 빼서 쓰고있진 않은지? million, billion을 잘 구분했는지?
```python
### 문제가 되는 부분 ###
df = df.sort_values(by='GDP (B USD)', ascending=False)
        with open('country_region_table.json', 'r', encoding='utf-8') as region_file:
            region_data = json.load(region_file)
        df['Region'] = df['Country'].map(region_data)
        log_message("Data Transformation Completed")
        return df
```
- 데이터를 저장하는 부분을 t단계로 옮기는 게 맞지 않을까? (save_gdp_data 등)
```python
# Extract
        extracted_data = extract_gdp_data(url, table_class)

        # Save Extracted Data
        save_gdp_data(extracted_data)

        # Transform
        transformed_data = transform_gdp_data(extracted_data)

        # Save Transformed Data
        save_gdp_data(transformed_data, 'transformed_gdp_data.csv', 'transformed_gdp_data.json')

        # Load into SQLite Database
        load_gdp_data(transformed_data)

        # Additional Analyses
        display_countries_with_gdp_over_100()
        display_region_top5_average_gdp()
```


> 오늘의 하루 리뷰
- 오늘은 특정 기술을 공부하거나 코드를 리팩토링 하는 것보다, "내가 그동안 진행해왔던 방법론"에 대한 생각을 깊게 해보았다.
- Dano님이 말씀해주셨던 것처럼, 내가 방향성을 먼저 정한 것이 아니라, 그냥 정해준 방향대로 앞만 보고 항해했던 것이 아닌가 생각되었다.
- 즉, 고민하는 시간이 특정 기술을 익히는 데에만 집중되다 보니, 다른 방향으로의 뇌 자극이 부족했던 것 같다.
- 문제를 정하고 방향을 정하는데에 시간을 더 많이 들여보도록 하자



---

## 🔍 회고 (KPT)

### ✅ Keep
- Jupyter Notebook을 제출할 때, 따로 정리를 해서 하나의 product로 만들어서 제출한 것 (그러나 내 기준에선 맞지만, dano님이 보셨을 때 원하셨던 형태의 Product인지는 잘 모르겠지만..)

### ⚠️ Problem
- 평가를 받는 것에 대한 두려움(?), 회피하는 자세를 고쳐야 한다. ==> 오늘 나는 성장할 기회를 놓쳤다. 코드 피드백을 신청했다면 성장하는 계단이 되었을 것. 이런 기회를 잡는 사람에게 보상이 주어질 것이다.
- 깃헙에 업로드시, 불필요한 파일까지 올린 것 ==> .gitignore을 잘 활용하지 못한 것

### 💡 Try
- W1M3를 수정하는 과정부터 .gitignore을 잘 활용하도록 해보자. 
- W1M3의 수정을 우선 순위로 하여, 첫 미션부터 지금까지 진행한 미션들에게 모두 '내가 아닌, 다른 사람, 그리고 타겟을 대상'으로 작성된 코드가 되도록 수정을 진행하기
- Use case 를 아주 구체적으로 써보기 ==> 주제를 좁히고, 고객을 좁게 정의하기

  
- **'나는 어제보다 더 나아질 것이다'** 라는 마음 꼭 간직하고 열심히 하자 재웅아!!

---
