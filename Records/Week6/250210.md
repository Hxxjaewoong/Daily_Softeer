# 📅 2025.02.10 리뷰 & 회고

---

# 📝 리뷰

## Apache Spark Job 최적화  

### 1⃣ Spark 최적화 경우  
- **컴퓨팅 효율성과 노드 간 통신 최소화**  
- **RDD vs DataFrame**  
  - DataFrame이 성능 최적화의 원천 (Catalyst Optimizer 적용 가능)
  - RDD는 메모리 제어 및 복잡한 연산에 유용
  - 비정형 데이터(텍스트, 바이너리 파일 등) 처리 시 RDD 활용  

---

### 2⃣ 데이터 저장 형식 및 압축 최적화  
- **Parquet, ORC(컬럼 저장 방식) + Snappy, LZ4 압축** → 성능 향상 및 저장 공간 절약  
- **컬럼 저장 방식(Columnar Storage)의 장점**  
  - ✅ **압축 효율** (유사한 데이터 타입에서 압축 최적화)  
  - ✅ **컬럼 단위 검색** (불필요한 컬럼 읽기 방지)  
  - ✅ **Aggregation 성능 향상** (단일 컬럼 연산 최적화)  
  - ✅ **Predicate Pushdown** → 불필요한 데이터 로딩 방지  

- **RDS vs Redshift 비교**  
  - **RDS**: 트랜잭션(OLTP), 행 기반 저장(row-based) 
  - **Redshift**: 분석(OLAP) 최적화, 컬럼 저장(columnar)  

---

### 3⃣ 메모리 관리 및 캐싱 전략  
- **Spark 메모리 구조**
  - **Storage Memory**: RDD 및 DataFrame 캐싱  
  - **Execution Memory**: Shuffle, Join, Sort 등 연산 중 생성되는 임시 데이터 저장  
  - **User Memory**: Spark 애플리케이션에서 사용하는 중간 데이터 저장  
  - **Reserved Memory**: Spark 내부 객체 및 JVM 관리에 사용  

- **Cache vs Persist vs Checkpoint 차이**  
  - `Cache`: 메모리에 저장, 재계산 가능 (Lineage 유지)
  - `Persist`: 특정 저장 레벨 지정 가능 (`MEMORY_AND_DISK` 기본값)  
  - `Checkpoint`: HDFS/S3에 저장, Lineage 제거 (장기 실행 Job에 유용)  

---

### 4⃣ Shuffle 최적화 및 데이터 스큐 해결  
- **Shuffle 발생 원인**  
  - `groupByKey`, `reduceByKey`, `join` 등에서 데이터 이동 발생  
  - 데이터 불균형(Data Skew)으로 특정 키에 데이터 집중  

- **최적화 방법**  
  - ✅ `sortWithinPartitions()` → 파티션 내부 정렬로 불필요한 Shuffle 최소화  
  - ✅ `repartitionByRange()` → 특정 컬럼 값 범위 기준으로 파티션 조정  
  - ✅ `coalesce(n)` → 기존 파티션 유지하면서 파티션 수 줄이기 (Shuffle 발생 X)  
  - ✅ `filter + agg` 활용 → `groupBy` 연산 최소화  

- **데이터 스큐 해결 방법**  
  - **Salting**: 키에 난수를 추가하여 데이터 균등 분배  
    ```python
    from pyspark.sql.functions import rand
    df = df.withColumn('salted_key', df['city'] + (rand()*100).cast("int"))
    ```
    - ⚠️ Salting 범위(`rand()*100`)는 데이터 크기에 맞게 조정 필요  
  - **Broadcast Join**: 작은 데이터셋을 브로드캐스트하여 Shuffle 최소화  
    ```python
    from pyspark.sql.functions import broadcast
    df1.join(broadcast(df2), 'id')
    ```
    - ⚠️ 작은 데이터셋(`df2`)이 클러스터 메모리에 적재 가능해야 함  
    from pyspark.sql.functions import broadcast
    df1.join(broadcast(df2), 'id')
    ```

---

# 🔍 회고 (KPT)

## ✅ Keep

- 점심 시간, 퇴근 시간 이후에도 꾸준히 부족한 부분을 채워가려고 한 것..

## ⚠️ Problem

- 일정을 따라가지 못한 것..
    - 하나의 일에 꽂혀서 다른 일을 못했다. 일정을 정해놓고 작업을 하는 만큼, 시간 관리 및 작업 분배에 더 신경을 쓰자.

---