# 📅 2025.02.11 리뷰 & 회고

---

# 📝 리뷰

## 아이디어 관련

주제 선정이 아직도 난항을 겪고 있다. 그래서 기본으로 돌아가서 최종 프로젝트의 주제를 하나하나 뜯어보았다.

### 신규 모델이 출시되었을 때

- 왜 신규 모델이 출시 되었을 때일까?
    - 신규 모델의 초기 판매량은 매우 중요하다
    - 초기 판매량은 우상향 그래프를 그리기 쉽지 않으므로, 초기 판매량을 유지하는 것이 중요하므로
    - 소비자 관심이 가장 집중되는 시기이다.
    - 신차 출시 직후에는 자동차 팬, 소비자, 자동차 리뷰어, 업계 전문가들이 가장 활발하게 반응하는 시기
    - 이슈 발생 가능성이 가장 높은 시점이기 때문이다.
    - 새로운 모델이 시장에 나왔을 때 예상치 못한 문제(예: 품질, 기능, 가격 등)에 대한 논란이 발생할 가능성이 크다.
    - 이런 이슈가 확산되기 전에 조기 탐지하여 대응하면 브랜드 신뢰도 유지에 유리

### SNS (유튜브, 인스타그램, 커뮤니티 등)에서

- 왜 SNS 반응을 봐야하는가??
    - 신차 출시 직후에는 자동차 팬, 소비자, 자동차 리뷰어, 업계 전문가들이 가장 활발하게 반응하는 시기인데, 관심도가 높아짐에 따라 SNS, 커뮤니티, 유튜브 등에서 다양한 의견이 빠르게 형성된다.
        - 출시 시점에는 콘텐츠(리뷰, 댓글, 게시글, 영상 등)가 폭발적으로 증가하기 때문에 분석에 필요한 데이터 확보가 쉽다.
    - SNS는 소비자의 의견이 가장 즉각적으로 반영되는 공간이다
        - SNS는 사람들이 제품이나 브랜드에 대해 가장 빠르고 직접적으로 의견을 공유하는 플랫폼이다.
    - 신차 출시 직후, 유튜브 리뷰, 인스타그램 게시물, 커뮤니티 게시글 및 댓글 등을 통해 소비자 반응이 실시간으로 나타난다.
    - **요즘은 광고 보다는 SNS를 믿는 분위기가 형성됨**
        - 자동차 구매자들은 이제 전통적인 광고보다 실제 사용자 리뷰, SNS 인플루언서 의견, 온라인 커뮤니티 토론을 더 신뢰하는 경향이 있다.
    - 전통적인 방법보다 저렴한 방식이다.
        - 전통적인 소비자 조사(설문조사, FGI 등)는 시간과 비용이 많이 들지만, SNS 분석은 실시간으로 방대한 데이터를 자동 수집·분석할 수 있어 비교적 효율적이다.

### 소비자들의 반응을 모니터링

- 왜 소비자 반응이 중요한가?
    - 신차 출시 직후 소비자들의 반응은 제품 성공 여부를 결정짓는 중요한 요소이다.
    - 예상치 못한 문제(예: 결함, 불편한 기능, 디자인 논란)가 발생할 경우, 이를 조기에 감지하고 신속하게 대응해야 브랜드 신뢰도를 유지할 수 있다.
    - 현대 자동차와 같은 글로벌 브랜드는 소비자들의 인식이 기업의 장기적인 성공에 중요한 영향을 미침
    - 실제 소비자들의 진짜 니즈를 파악할 수 있다.
        - 자동차 업계에서는 종종 내부적인 가설에 기반한 제품 기획이 이루어지지만, 실제 소비자들이 원하는 기능과 기대하는 점이 다를 수 있음
        - SNS 및 커뮤니티에서 소비자들이 직접 언급하는 불만 사항과 개선 요청을 분석하면, 이후 모델 개발 및 마케팅 전략을 보다 소비자 중심으로 조정할 수 있음
    - 마케팅 전략에 대한 방향성 설정
        - 소비자들의 반응을 분석하여 어떤 점을 강조할지, 어떤 메시지를 전달할지 최적화 가능
    - 위기 대응
        - 제품 출시 후 가장 우려되는 것은 예상치 못한 문제(리콜, 품질 결함 등)가 발생했을 때 기업이 늦게 대응하는 것
        - 실시간으로 소비자들의 불만 사항을 추적하면, 문제가 커지기 전에 빠르게 대응할 수 있음

### 특정 이슈에 대한 대화가 얼마나 이루어지고 있는지를 모니터링

- 이슈의 중요도 를 파악하기 위함이다.
    - 단순히 의견을 수집하는 것보다는 특정 이슈가 얼마나 많은 사람들에게 회자되고 있는지를 분석해야한다.
    - 문제의 심각성과 기업이 대응할 우선순위를 매길 수 있다.
- 이슈의 양? 이 아닌 맥락이 중요하기 때문이다.
    - 단순히 의견이 얼마나 이루어졌냐도 중요하지만, 특정 이슈가 왜 이루어졌는지도 알 수 있기 때문
- 현재 시장의 트렌드와 패턴 분석이 가능하므로

### 알림을 제공

- 왜 알림을 줘야하는가?
    - 즉각적인 반응은 문제 해결에 매우 중요함
    - 예를 들어, 특정 부품의 결함이나 신차 디자인에 대한 부정적인 반응이 급격히 확산될 경우, 빠른 시간 내에 **알림을 통해 해당 이슈를 담당자에게 전달**함으로써 즉시 대응할 수 있음
    - 비즈니스 리스크를 최소화 가능
    - 즉시 소비자에게 관련 사항을 안내하거나, 해결 방안을 제공할 수 있음

## 커뮤니티를 크롤링해보기

- 디시인사이드와 클리앙이라는 커뮤니티를 ‘아반떼’라는 키워드를 가지고 크롤링을 해보았다.

```python
# 클리앙

# 여러 페이지 크롤링
for page in range(0, max_pages):
    print(f"📌 현재 {page + 1} 페이지 크롤링 중...")

    # 검색 결과 페이지 URL 생성
    search_url = base_url.format(search_keyword, page)

    # 페이지 요청
    response = requests.get(search_url, headers=headers)
    
    if response.status_code != 200:
        print(f"❌ 페이지 {page} 요청 실패 (상태 코드: {response.status_code})")
        break

    # BeautifulSoup 객체 생성
    soup = BeautifulSoup(response.text, 'html.parser')

    # 게시글 리스트 추출
    posts = soup.select("div.list_item")

    for post in posts:
        board_name = post.select_one("button.shortname.fixed")  # 게시판명
        title_tag = post.select_one("a.subject_fixed")  # 게시글 제목
        author = post.select_one("span.nickname span")  # 작성자
        date = post.select_one("div.list_time span.timestamp")  # 작성 날짜
        views = post.select_one("div.list_hit span.hit")  # 조회수
        comments = post.select_one("a.list_reply span.rSymph05")  # 댓글 수

        if title_tag and board_name:
            post_url = "https://www.clien.net" + title_tag['href']  # 게시글 URL

            print(f"🔗 게시글 방문: {title_tag.text.strip()}")
            print(f"   👉 URL: {post_url}")

            # 게시글 페이지 방문하여 본문과 좋아요 수 가져오기
            post_response = requests.get(post_url, headers=headers)
            if post_response.status_code != 200:
                print(f"⚠️ 게시글 요청 실패: {post_url}")
                continue

            post_soup = BeautifulSoup(post_response.text, 'html.parser')

            # 본문 추출
            content_tag = post_soup.select_one("div.post_article")
            content = "\n".join([p.text.strip() for p in content_tag.find_all("p")]) if content_tag else "본문을 가져올 수 없음"

            # 좋아요 수 추출
            like_tag = post_soup.select_one("a.symph_count.disable strong")
            likes = like_tag.text.strip() if like_tag else "0"  # 좋아요 수가 없으면 0으로 처리

            # 데이터 저장
            board_list.append({
                "게시판": board_name.text.strip(),
                "제목": title_tag.text.strip(),
                "URL": post_url,
                "작성자": author.text.strip() if author else "N/A",
                "작성일": date.text.strip() if date else "N/A",
                "조회수": views.text.strip() if views else "N/A",
                "댓글 수": comments.text.strip() if comments else "0",
                "좋아요 수": likes,
                "본문": content
            })

            time.sleep(2)  # 차단 방지를 위해 대기

```

```python
# 디시 
# 멀티프로세싱 이용

# 결과 저장 리스트
results = []

# 게시글 상세 정보를 가져오는 함수
def fetch_post_details(post_url):
    try:
        post_response = requests.get(post_url, headers=headers, timeout=5)
        if post_response.status_code != 200:
            return None

        post_soup = BeautifulSoup(post_response.text, 'html.parser')

        # 게시글 제목
        title_tag = post_soup.find("span", class_="title_subject")
        title = title_tag.get_text(strip=True) if title_tag else "제목 없음"

        # 게시글 내용
        content_tag = post_soup.find("div", class_="write_div")
        content = content_tag.get_text(strip=True) if content_tag else "내용 없음"

        # 게시글 날짜
        date_tag = post_soup.find("span", class_="gall_date")
        date_str = date_tag.get_text(strip=True) if date_tag else "날짜 없음"
        try:
            post_date = datetime.strptime(date_str, "%Y.%m.%d %H:%M:%S")
            post_date = post_date.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            post_date = "날짜 변환 실패"

        # 작성자
        nickname_tag = post_soup.find("span", class_="nickname")
        nickname = nickname_tag.get_text(strip=True) if nickname_tag else "익명"

        # IP 주소
        ip_tag = post_soup.find("span", class_="ip")
        ip = ip_tag.get_text(strip=True).replace("(", "").replace(")", "") if ip_tag else "IP 없음"

        # 추천 수
        recommend_tag = post_soup.find("p", class_="up_num")
        recommend_count = recommend_tag.get_text(strip=True) if recommend_tag else "0"

        # 비추천 수
        dislike_tag = post_soup.find("p", class_="down_num")
        dislike_count = dislike_tag.get_text(strip=True) if dislike_tag else "0"

        # 댓글 수
        comment_tag = post_soup.find("span", id=lambda x: x and x.startswith("comment_total"))
        comment_count = comment_tag.get_text(strip=True) if comment_tag else "0"

        # 조회수
        view_count_tag = post_soup.find("span", class_="gall_count")
        view_count = view_count_tag.get_text(strip=True).replace("조회 ", "") if view_count_tag else "0"

        return {
            "title": title,
            "url": post_url,
            "date": post_date,
            "author": f"{nickname} ({ip})",
            "content": content,
            "views": view_count,
            "recommend": recommend_count,
            "dislike": dislike_count,
            "comments": comment_count
        }
    except Exception as e:
        print(f"게시글 {post_url} 크롤링 오류: {e}")
        return None

# 여러 페이지의 게시글을 가져오는 함수
def fetch_page_posts(page):
    search_url = base_url.format(page)
    response = requests.get(search_url, headers=headers, timeout=5)
    
    if response.status_code != 200:
        print(f"페이지 {page} 요청 실패 (상태 코드: {response.status_code})")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    posts = soup.select("ul.sch_result_list li")

    post_urls = []
    for post in posts:
        title_tag = post.find("a", class_="tit_txt")
        if title_tag:
            post_urls.append(title_tag["href"])

    return post_urls

# 병렬 크롤링 실행
with ThreadPoolExecutor(max_workers=10) as executor:  # 10개의 스레드 사용
    # 1. 게시글 목록 가져오기 (각 페이지 크롤링)
    future_to_page = {executor.submit(fetch_page_posts, page): page for page in range(1, max_pages + 1)}

    all_post_urls = []
    for future in as_completed(future_to_page):
        post_urls = future.result()
        if post_urls:
            all_post_urls.extend(post_urls)

    print(f"총 {len(all_post_urls)} 개의 게시글을 찾았습니다.")

    # 2. 게시글 상세 정보 가져오기 (병렬 실행)
    future_to_post = {executor.submit(fetch_post_details, post_url): post_url for post_url in all_post_urls}

    for future in as_completed(future_to_post):
        post_data = future.result()
        if post_data:
            results.append(post_data)

# DataFrame으로 변환
df = pd.DataFrame(resul
```

```python
# 디시
# 멀티 프로세싱 이용 x

# 결과를 저장할 리스트
results = []

# 여러 페이지 순회
for page in range(1, max_pages + 1):
    print(f"현재 {page} 페이지 크롤링 중...")

    # 페이지별 검색 URL 생성
    search_url = base_url.format(page)

    # 페이지 요청
    response = requests.get(search_url, headers=headers)
    if response.status_code != 200:
        print(f"페이지 {page} 요청 실패 (상태 코드: {response.status_code})")
        break

    soup = BeautifulSoup(response.text, 'html.parser')

    # 게시글 목록 추출
    posts = soup.select("ul.sch_result_list li")

    # 게시글이 없으면 종료
    if not posts:
        print("더 이상 게시글이 없습니다. 크롤링 종료.")
        break

    for post in posts:
        # 게시글 제목과 URL
        title_tag = post.find("a", class_="tit_txt")
        if title_tag:
            title = title_tag.get_text(strip=True)
            post_url = title_tag["href"]
        else:
            continue

        # 게시글 상세 페이지 요청
        post_response = requests.get(post_url, headers=headers)
        if post_response.status_code != 200:
            print(f"게시글 {post_url} 요청 실패, 건너뜀")
            continue

        post_soup = BeautifulSoup(post_response.text, 'html.parser')

        # 게시글 내용
        content_tag = post_soup.find("div", class_="write_div")
        content = content_tag.get_text(strip=True) if content_tag else "내용 없음"

        # 게시글 날짜
        date_tag = post_soup.find("span", class_="gall_date")
        date_str = date_tag.get_text(strip=True) if date_tag else "날짜 없음"
        try:
            post_date = datetime.strptime(date_str, "%Y.%m.%d %H:%M:%S")
            post_date = post_date.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            post_date = "날짜 변환 실패"

        # 추천 수
        recommend_tag = post_soup.find("p", class_="up_num")
        recommend_count = recommend_tag.get_text(strip=True) if recommend_tag else "0"

        # 비추천 수
        dislike_tag = post_soup.find("p", class_="down_num")
        dislike_count = dislike_tag.get_text(strip=True) if dislike_tag else "0"

        # 댓글 수
        comment_tag = post_soup.find("span", id=lambda x: x and x.startswith("comment_total"))
        comment_count = comment_tag.get_text(strip=True) if comment_tag else "0"
        
        # 조회수
        view_count_tag = post_soup.find("span", class_="gall_count")
        view_count = view_count = view_count_tag.get_text(strip=True).replace("조회 ", "") if view_count_tag else "0"

        # 작성자 닉네임
        nickname_tag = post_soup.find("span", class_="nickname")
        nickname = nickname_tag.get_text(strip=True) if nickname_tag else "익명"

        # 작성자 IP
        ip_tag = post_soup.find("span", class_="ip")
        ip = ip_tag.get_text(strip=True).replace("(", "").replace(")", "") if ip_tag else "IP 없음"

        # 결과 저장
        results.append({
            "title": title,
            "url": post_url,
            "date": post_date,
            "content": content,
            "author": f"{nickname} ({ip})",  # 작성자 + IP 추가
            "views": view_count,
            "recommend": recommend_count,
            "dislike": dislike_count,
            "comments": comment_count
        })

        # 요청 사이 딜레이 추가 (과부하 방지)
        time.sleep(1)
```

확실한 Use Case가 없어서 그런지 조금 막막하다.. 또한 멀티 프로세싱을 이용해서 크롤링을 했더니 IP가 막혀버렸다. 이럴 땐 어떻게 대처해야할지는 더 공부와 방법론에 대한 연구가 필요할 듯 하다..

---

# 🔍 회고 (KPT)

## ✅ Keep

- 오늘은 진행 사항을 모두 기록했다. 물론 평소에도 기록을 했지만, 오늘은 특히 모든 과정 (생각 + 작업)과 고민했던 것, 스쳐갔던 것 등 정말 모든 것을 적었다. 그랬더니 퇴근 전에 리뷰를 진행했을 때 글들을 쭉 읽어보니, 확실히 생각이 뻗어갈 수 있는 가지가 많아졌음을 느꼈다. 내일 아침 스크럼 때 팀원들에게 이 생각을 정리해서 보여줘야겠다.

## 💡 Try

- 과도한 requests로 인한 IP가 막혔을 때 어떻게 대처하고 다른 방식은 무엇이 있을지 고민해보자…
---
