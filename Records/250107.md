# 📅 2025.01.07

---
## Scrum


#### 오전 미팅 내용
- 파이썬으로 transform 하는 게 아니라 데이터 프레임을 활용해서 transform 했는가?  
    * 우리는 row에 값을 추가할 때 코드로 하나하나 추가하는 코드를 작성했다. 이 과정을 데이터 프레임으로 활용할 순 없을까?
    * 그렇다면 어제 팀 미팅에서 말했던 'Transform의 단계를 더 세분화'와 비슷한 과정인 것일지도?


#### 오늘 할 일 (팀 전체)  
- [✔️] 오전 중에는 코드 리팩토링 (dano님 피드백 반영)  
    * 파이썬으로 transform 하는 게 아니라 데이터 프레임을 활용해서 transform 했는가?  
    * 코드는 외부의 변화(i.e. IMF 웹페이지의 변경)에 쉽게 대응가능한 (유지 보수가 용이한) 형태의 코드인가?
- [✔️] 오후 중에는 추가 요구사항 반영한 코드 작성해보기



#### 내 할일
- [✔️] Transform이 이루어지지 않은 데이터를 json에 저장하도록 코드 변경
- [✔️] Extact한 데이터를 바로 처리(Transform)하지 않고 저장하는 이유가 뭘까? 어떤 경우에 그렇게 해야 할까?
- [✔️] 내가 만든 코드가 Pandas DataFrame의 장점을 **충분히** 활용한 코드인지 확인해보기
- [✔️] 코드는 외부의 변화(i.e. IMF 웹페이지의 변경)에 쉽게 대응가능한 (유지 보수가 용이한) 형태의 코드인지 확인해보기

---


# 리뷰 & 회고

## 📝 리뷰

### 학습 내용 
1. M3를 다시 천천히 읽어보면서 요구사항 재정립 해보기
    - 내가 만들어야 하는 것
        1. 국가별 GDP 확인 가능 테이블 (GDP의 단위는 1B USD이어야 하고 소수점 2자리까지만) (GDP가 높은 국가들이 먼저)
        2. GDP가 100B USD이상이 되는 국가만을 구해서 화면에 출력
        3. 각 Region별로 top5 국가의 GDP 평균을 구해서 화면에 출력
        4. 추출 (Extract)한 정보는 'Countries_by_GDP.json'라는 이름의 JSON 화일 포맷
            (Transform이 이루어지지 않은 데이터를 json에 저장하도록 변경)
    - 지켜야 하는 것
        1. 정보가 갱신되더라도 해당 코드를 재사용해서 정보를 얻을 수 있어야 함
        2. 주석 쓰기 
    - ETL Process에 대해 다시 생각해보기
        - ETL과 ELT는 다른 것
        - Extract는 raw data를 뽑아내는 것인가?

2. 리팩토링은 끝이 없는 것 같다. 비록 힘이 들기는 하지만 이는 좋은 것 같다.
3. dano님이 첫날에 보여주셨던 잼 바르는 비디오... 매우 맞는 말이었다. 요구사항은 정확해야한다. 그래야 팀원들에게 정확히 요청하고 받을 수 있는 것 같다.


#### 오늘 한 내용
1. W1M3 추가요구사항 전까지의 코드 중 version3-6 작성
    1. load_gdp_data()에서 작동하는 CSV 파일로 저장하는 것과 JSON 파일로 저장하는 것을 load_gdp_data()에서 수행하는 것이 아니라, save_gdp_data() 함수로 뺌
    2. 원래의 Load 과정에 있던 저장 프로세스를 함수로 뺌
    3. Pandas의 DataFrame을 더 활용하고, 변화에 대응해보도록 노력
    4. 추가 요구사항에 맞춘 쿼리 적용 함수 추가
2. 생각해볼 것
    - 다른 파일 (ex. dummytest.csv)이 들어왔을 때, 어떻게 처리할 것인지에 대해 고민
3. 같이 생각해본 것
    - 작성한 코드는 Pandas DataFrame의 장점을 충분히 활용한 코드인가?  
    : BeautifulSoap을 활용해 추출한 내용을 DataFrame으로 활용해서 테이블을 만들지 않고, 파이썬 코드 레벨로 만든 것이 아쉬우니 고쳐보자
    - 작성한 코드는 외부의 변화 (i.e. IMF 웹페이지의 변경)에 쉽게 대응가능한 (유지 보수가 용이한) 형태의 코드인가?  
    : 구조 변경에 어떻게 대응할 것인가?
4. 내 코드의 문제점
    1. 테이블 구조에 의존적  
        - find('table', {'class': 'wikitable'})는 HTML의 특정 클래스 이름에 의존하므로, 테이블 클래스가 변경되면 실패할 가능성이 큼  
        - df.iloc[:, [0, 1, 2]]는 칼럼 순서에 의존하므로, 칼럼 순서가 바뀌면 잘못된 데이터를 선택할 위험이 있음.

    2. 데이터 유효성 검증 부족  
        - 스크래핑한 데이터가 예상 구조를 따르지 않을 경우 오류를 유발할 수 있음.
        - 데이터 변환 단계에서 예외 처리 부족.
        
    3. 하드코딩된 URL  
        - URL이 변경되면 전체 ETL 프로세스가 중단됨.
5. 해결 방안이 뭐가 있을까?
    1. HTML 구조 변경 대응
        - CSS 선택자 활용: BeautifulSoup의 CSS 선택자를 사용하여 더 유연한 테이블 선택이 가능하도록 변경합니다.
        - 테이블 구조 검증: 테이블이 올바르게 선택되었는지 확인하는 코드를 추가합니다.
        - 칼럼 이름 기반 처리: iloc 대신 칼럼 이름으로 데이터 선택을 진행하여 칼럼 순서 변경에 대응합니다.

    2. 데이터 유효성 검증
        - 테이블의 존재 여부와 칼럼 이름 확인.
        - 예상 데이터 형식(숫자, 문자열 등)을 확인하고, 문제가 있는 데이터를 로그로 기록하거나 무시.

    3. URL 변경 대응
        - URL을 외부 설정 파일이나 환경 변수로 관리하여 하드코딩을 제거.

    4. 오류 처리 및 로깅 강화
        - 각 단계별 오류를 캡처하고, 원인을 로그로 기록
6. 그에 따라 내가 고쳐본 것
    1. 설정 파일 관리:
        - load_config 함수로 config.ini 파일에서 URL과 HTML 테이블 클래스 값을 로드
        - 설정값 누락이나 파일 부재 시 예외를 발생시켜 문제를 방지
    2. 로깅 강화:
        - 로그 메시지에 수준(INFO, ERROR) 추가
        - 주요 단계마다 로그 기록으로 문제 발생 시 추적이 쉬워짐 !
    3. 예외 처리 강화:
        - HTTP 요청, 데이터 추출, 변환 등 주요 단계에서 예외 처리 추가함
    4. 코드의 재사용성 향상:
        - URL과 테이블 클래스 정보를 외부 파일에서 관리하여 유지보수성 및 유연성 높임

---

## 🔍 회고 (KPT)

### ✅ Keep
- 끝없는 리팩토링
    - 팀원들과 코드 공유 후, 문제점이 될만 한 것들을 지적받기 + 팀원을 참고하여 내 코드에 고칠 점을 생각해보기
- 어제 다짐했던 바로바로 메모하는 것 (ex. 팀원들의 생각 공유 때 바로 메모하고 토의 후 적용해보기)

### ⚠️ Problem
- 해당 주차에 배웠던 내용을 중점적으로 사용해보는 노력이 필요한 것 같다
    - ex. 이번주 M1에 학습했던 Pandas의 DataFrame을 더 활용하는 방안으로 M3를 진행했어야 했다. 
- 코드의 유연성에 대한 문제점과 내 생각..
    - 너무 주제에만 맞게 짠 코드가 아닌가..?
    - 그렇다면 더 넓은 범위로 usable한 코드는 어떻게 작성할 것인가..
    - 근데 또한 use의 범위는 어디까지인가...
- 실패 및 망가짐에 대한 두려움
    - 왜 전체 짜임새가 틀어질까 무서워서 extract_gdp_data()을 최적화하지 않았던가....
    - 하니까 되던데..

### 💡 Try
- 용기 있게 도전해보기 (틀릴 용기, 실수할 용기)
    - 여러 방면으로 리팩토링 해볼 줄 알아야한다. 
    - 될까 말까 고민할 때 일단 해보는 것
- 배웠던, 학습했던 것들 위주로 적극적으로 활용해보기
- 작성한 M3 최종 코드가 지금보다 훨씬 큰 대용량 데이터셋에서는 어떨지 생각해보기
- 민재님께서 퇴근 전 진행한 코드 공유 및 리뷰 시간에서 대용량 데이터셋에 대해 과정별로(ETL) 어떤 작업(파싱은 어떤 작업을 사용했는자, 청크 단위로 얼마나 쪼갤 수록 달라지는지 등)을 했을 때 최적화가 됐는지 실험한 것을 보여주셨다
    : 매우매우매우 유익했음
    : 이러한 방향으로 생각을 열 줄 알아야함 ⭐️⭐️⭐️⭐️⭐️

---