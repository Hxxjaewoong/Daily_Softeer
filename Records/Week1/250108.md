# 📅 2025.01.08

---
## Scrum

#### 오전 미팅 내용
- 다노님의 회고 피드백에 대한 생각 공유해보기
    1. 우리가 작성한 코드를 ETL로 잘 나눠 보기. 정의대로 잘 나누어지는가?
        - 우리가 작성한 코드는 ETL 각각의 단계가 독립적으로 구분되어 작동하는가?
        - 만약 ETL에서 각각의 과정을 별도로 프로세스로 처리한다면 어떻게 코드를 변경해야 할까?
        - 병렬/분산 처리를 한다면 대응 가능한 코드인가?
        - 대용량 데이터를 처리하는 과정에서 에러가 난다면 어떻게 해야 할까?
    2. “ETL과정에 대해 생각한 정의가 팀원과 달랐다. 웹을 크롤링해와서 데이터프레임으로 만드는 과정 자체를 Transform으로 보아야할까? 그럼 데이터프레임은 원시데이터가 아닌것일까?” ==> 우리의 생각은?
    3. “추출 (Extract)한 정보는 'Countries_by_GDP.json'라는 이름의 JSON 화일 포맷으로 저장해야 합니다.” ==> 추출한 데이터를 저장했을 때 어떤 이점이 있을까?
- 이에 따라 우리의 미션을 다시 뒤돌아보도록 하자

#### 오늘 할 일 (팀 전체)  
- [✔️] 각자의 코드와 다노님의 회고 피드백을 같이 보며 각자 생각 정리하기  
- [✔️] 각자의 생각을 공유하고, 서로에게 피드백 해주기
- [✔️] M1 부터 M3까지 리뷰 및 리팩토링
    

#### 내 할일
- [✔️] M1부터 M3까지 복습하고 리팩토링 해보기
- [✔️] 회고 피드백에 대한 내 생각 정리 및 팀원들의 피드백 듣기

---


# 리뷰 & 회고


---


## 📝 리뷰 및 학습내용

### 학습 내용

> Week1의 M1 부터 M3까지 다시 코드를 작성해보며 복습했다. 운동이든 공부든 기본기가 제일 중요한 법...

#### 우리가 작성한 코드를 ETL로 잘 나눠 보기. 정의대로 잘 나누어지는가?
> - 우리가 작성한 코드는 ETL 각각의 단계가 독립적으로 구분되어 작동하는가?
>> - 나의 코드
>>      1. Extract에서 데이터를 URL로부터 수집하고, HTML 구조를 파싱하여 필요한 데이터를 추출했다. 여기서 필요한 칼럼만 데이터를 뽑아왔는데, GDP를 내림차순으로 정렬하고 소수점 2자리로 반올림 한 값으로 변경했다. 그렇다면 내림차순 및 반올림 과정도 Extract의 해당되는 것일까? 정의하기 나름인 것일까?
>>      2. Extract에서 위키피디아에 있는 데이터를 그대로 가져오는 과정을 거쳤다. 그렇다면 나는 이것을 Raw Data라고 생각한 것이다. 그리고 원래 위키피디아에는 따로 'Region'에 대한 정보가 없었기 때문에, 다른 json과 매핑하여 정보를 가공했다. 그러므로 나는 이것을 Transform이라고 생각했다. 왜냐하면 Raw Data에 다른 정보와 '가공'이 들어간 것이니까. 근데 그렇게 따지면, Extract에서 반올림한 과정도 Transform이 될 수도 있지 않을까? 어디까지의, 또한 어느 정도의 과정까지를 E, T로 구분할 것인가..
>>      3. 필터링해서 출력하는 과정등을 나는 Transform이 아니라 따로 함수를 만들어서 사용했다. 그런데 이런 과정도 어떻게 생각해보면 Transform이 될 수도 있지 않는가? 아니 어쩌면 맞는 것 같기도 하다.
>>      4. Load 과정은 데이터 베이스에 연결하여 데이터를 저장하는 프로세스를 넣어두었다. 다른 과정은 없어도 괜찮은 것일까? 

> - 만약 ETL에서 각각의 과정을 별도로 프로세스로 처리한다면 어떻게 코드를 변경해야 할까?
>> - 나의 생각
>>      - 사실 질문 자체를 정확히 파악하진 못한 것 같다. 그렇다면 질문부터 정의해보자.
>>      - 별도의 프로세스로 처리 -> 각 단계(Extract, Transform, Load)를 독립된 실행 단위로 나눠 실행한다는 것?
>>      - 아니면 각 단계 안에서 여러 데이터 소스를 동시에 추출하거나, 데이터 변환을 청크 단위로 분리해 처리한다는 의미일까?
>>      - 후자라면 어떻게 코드를 변경할 것인가?
>>      - 각 단계 안에서 여러 데이터 소스를 동시에 추출하거나, 데이터 변환을 청크 단위로 분리해 처리한다는 의미 같은데, 그렇다면 다음 질문과 바로 이어지는 것 같다.
>>      - Extract 단계에서는 여러 데이터 소스에서 병렬로 추출해서 속도를 높일 수 있을 것 같다.
>>      - Transform에서는 대용량 데이터를 효율적으로 처리하기 위해, 데이터를 청크 단위로 나눠 변환 작업을 수행하면 될 것 같다. (Pandas의 chunksize 옵션 고려)
>>      - 대규모 데이터를 데이터베이스에 적재할 때 한 번에 처리하는 대신, 배치 크기를 설정하여 적재 속도와 안정성을 개선하는 방향

> - 병렬/분산 처리를 한다면 대응 가능한 코드인가?
>> - 나의 생각
>>      - 내 코드는 대응이 어려운 코드인 것 같다
>>      - Extract에서는 현재 단일 데이터 셋만을 활용하고 있는데, 여러 데이터 셋이 들어온다면 병렬/분산이 필수일 것 같다 (그래도 함수를 입력기준으로 따로 정의해둬서 확장은 용이할 것 같다)
>>      - 청크 단위로 데이터를 나눠 병렬 처리하려면, 데이터를 분리한 후 각 청크에 대해 독립적인 변환 작업을 수행하도록 수정이 필요할 것 같다

> - 대용량 데이터를 처리하는 과정에서 에러가 난다면 어떻게 해야 할까?  
>> - 나의 생각
>>      - 파악하기 위한 로깅이 정말 중요함을 실험 때 느꼈다. 정보를 잘 출력할 수 있어야함 (예. 추출 중 일부 데이터를 가져오지 못한 경우 해당 데이터만 로그에 기록)
>>      - 재시도 로직이나 청 시간 초과를 방지하기 위해 적절한 타임아웃 설정하는 방법도 있을 듯?

#### “ETL과정에 대해 생각한 정의가 팀원과 달랐다. 웹을 크롤링해와서 데이터프레임으로 만드는 과정 자체를 Transform으로 보아야할까? 그럼 데이터프레임은 원시데이터가 아닌것일까?” ==> 우리의 생각은?  

> 나의 생각
>>       : 나는 정의하기 나름이라고 생각했다. 나 같은 경우엔 데이터를 “가져오는” 것이 핵심 작업이라고 생각해서, 크롤링 및 데이터프레임 생성은 Extract로 간주했다. 그럼에도 불구하고 데이터 프레임의 생성 과정에서 어느 과정까지를 범주에 둘 것인가에 대한 논의가 팀원들과 필요한 것 같다.   

> 결국은 팀원과의 합의가 중요한 것 같다. ⭐️
>>  :팀의 공통된 정의를 정립하여 모든 작업자가 같은 기준으로 ETL을 이해하도록 하여야 할 것이다.


#### “추출 (Extract)한 정보는 'Countries_by_GDP.json'라는 이름의 JSON 화일 포맷으로 저장해야 합니다.” ==> 추출한 데이터를 저장했을 때 어떤 이점이 있을까?
> 나의 생각
> >     : "말 그대로 Raw한 데이터이기 때문에, 여러 방면으로 가공할 수 있는 유연함이 있지 않을까? 마치 어떤 요리를 할까 고민하기 전에 일단 생선부터 잡은 느낌이랄까" 그리고 또한 Raw한 JSON은 텍스트 기반 포맷으로 비교적 가볍고 간단하여, 데이터프레임으로 로드하거나 특정 키에 접근하는 데 시간이 적게 소요될 것이다.그리고 또한 추출한 데이터가 올바르게 크롤링되었는지, 구조에 이상이 없는지 검증하기 쉬우므로 좋다.


### ✊🏻 리뷰

##### 다노님 피드백에 대한 우리의 생각을 나눈 것에 대한 리뷰

> 우리가 작성한 코드를 ETL로 잘 나눠 보기. 정의대로 잘 나누어지는가?
- 프로세스를 나누더라도, 독립적이진 않을 것이다. 왜냐하면 ETL단계가 의존성을 서로 갖기 때문이다. 만약 의존성을 깨야한다면, 프로세스들이 데이터를 공유해야하는데, 이러한 에러는 또 어떻게 처리 할 것인가?
- 각 단계에서 마지막에 파일을 정리하고, 각 단계에서 파일을 들여다 보는 그런 프로세스를 구성해도 괜찮을 것 같다. 하지만 읽고 쓰기 과정의 빈도가 늘어나는 것에 대한 느려짐은 어떻게 해결할 것인지에 대해 고민해보도록 하자.
- 결국 각각의 파일(프로세스)들을 정의해서 데이터를 어떻게 관리할 것인지가 중요할 것 같다. 
- 그렇다면 대용량 처리 과정에서 에러가 난다면 어떻게 해야할 것인가? 결국은 리소스 (데이터)를 보는 것이기 때문에, 리소스 즉 메모리를 잘 계속해서 주시하고 관리하는 것이 중요할 것 같다.속해서 주시하고 관리하는 것이 중요할 듯


> “ETL과정에 대해 생각한 정의가 팀원과 달랐다. 웹을 크롤링해와서 데이터프레임으로 만드는 과정 자체를 Transform으로 보아야할까? 그럼 데이터프레임은 원시데이터가 아닌것일까?” ==> 우리의 생각은?  
- 서로 어디까지가 e까지 인지 매우 의견이 달랐다. 그래서 우리의 결론은 팀원과의 합의가 중요한 것 같다는 결론을 내렸다. 팀의 공통된 정의를 정립하여 모든 작업자가 같은 기준으로 ETL을 이해하도록 하여야 할 것이다. 다노님께서 이러한 의도로 아마 여쭤보시지 않았을까..?


> “추출 (Extract)한 정보는 'Countries_by_GDP.json'라는 이름의 JSON 화일 포맷으로 저장해야 합니다.” ==> 추출한 데이터를 저장했을 때 어떤 이점이 있을까?
- 다시 패칭을 안해도 된다
- 과정간의 독립성 확보를 위해 파일로 저장하는 것? 정도?
- 결국에는 유연성을 확보할 수 있다는 것이 가장 큰 장점이 아닐까 싶다

---

## 🔍 회고 (KPT)

### ✅ Keep
- 팀원들끼리 코드를 공유하니, 하나의 동작에서 여러개의 문법을 응용한 다양한 조합들이 나왔다. 팀원들과 코드를 공유하고, 그들의 코드를 나도 실행해보는 과정에서 문법이나 함수 등을 체득할 수 있는 과정이 좋았다.
- 오늘은 코드 공부보다는 '생각함'을 위주로 공부를 진행했다. 생각의 깊이가 깊어지도록 시간을 투자하니, 다양한 생각을 해볼 수 있고, 회로가 다양해져서 매우 유익했다. 이러한 과정도 꼭 공부하면서 넣도록 할 것

### ⚠️ Problem
- 함수 및 문법 등을 사용할 때 디폴트 형태를 익히지 않고, 필요한 부분만 사용한 것. (모르는 것을 사용할 때는 그 전에 항상 먼저 정확히 모르는 것에 대한 자세한 dictionary를 보고 적용하는 습관을 들여야 다른 상황에서도 유연하게 적용이 가능하다.)


### 💡 Try
- ETL 프로세스에서 병렬/분산 처리를 한다면 대응 가능한 코드로 변경해보기
- 몰랐던 내용은 사용하기 전에 항상 먼저 정확히 모르는 것에 대한 자세한 dictionary를 보고 적용하는 습관 들이기
---