# Hyundai Softeer Wiki

# 📅 2025.01.30 리뷰 & 회고

---

# 📝 리뷰

## Team Project

- 선택한 팀의 모든 팀원의 url을 모아서 딕셔너리 형태로 저장 ⇒ **선수 이름과 URL을 쉽게 매핑하고, 루프를 통해 크롤링**
- 생각보다 매우 번거러웠던 작업.. 내일 팀원들을 만나면 더 쉽고 자동화할 수 있는 방법이 있을지에 대해 논의해볼 예정

```python
players = {
    "Emiliano Martínez": {
        "premierleague": "https://www.premierleague.com/players/4245/Emiliano-Martínez/stats",
        "understat": "https://understat.com/player/4401"
    },
    "Robin Olsen": {
        "premierleague": "https://www.premierleague.com/players/11224/Robin-Olsen/stats",
        "understat": "https://understat.com/player/6962"
    },
    ....
```

## Mission

### 어제 해결하지 못하였던 pi 작업 결과물을 저장하는 데 성공

```python
from pyspark.sql import SparkSession
import os
import random

def inside(_):
    x, y = random.random(), random.random()
    return 1 if x*x + y*y < 1 else 0

if __name__ == "__main__":
    spark = SparkSession.builder.appName("Pi Estimation").getOrCreate()

    n = 1000000  # 100만 개의 샘플
    count = spark.sparkContext.parallelize(range(n)).map(inside).reduce(lambda a, b: a + b)
    pi_estimate = 4.0 * count / n

    output_dir = "/opt/spark/output"
    os.makedirs(output_dir, exist_ok=True)

    df = spark.createDataFrame([(pi_estimate,)], ["Estimated Pi Value"])

    #  CSV 파일로 저장
    csv_output_path = os.path.join(output_dir, "pi_estimate.csv")
    df.write.mode("overwrite").csv(csv_output_path, header=True)

    #  Parquet 파일로 저장
    parquet_output_path = os.path.join(output_dir, "pi_estimate.parquet")
    df.write.mode("overwrite").parquet(parquet_output_path)

    print(f"Estimated Pi Value: {pi_estimate}")
    print(f"CSV Result saved to {csv_output_path}")
    print(f"Parquet Result saved to {parquet_output_path}")

    spark.stop()
```

### Docker를 활용한 Spark를 Jupyter notebook을 통해 작업해보기

```yaml
command: >
  bash -c "pip install jupyter pandas matplotlib && 
  jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' & 
  /start-master.sh"
```

- 해당 코드를 docker compose에 추가함으로써 로컬에서 8888포트에서 jupyter notebook을 실행했다.
  - ⇒ 마운트를 활용하여 작업물을 남기도록 함
- 해결 및 팀원과 논의 해야할점..
  1. 하나하나 사이트에서 다운받은 데이터를 활용하고 있는데, 자동으로 다운받을 방법이 있을까?
  2. Spark라는 굉장한 툴을 활용해서 작업을 수행하고 있는 것인가?

```python
from pyspark.sql import SparkSession

# Spark 세션 생성
spark = SparkSession.builder.appName("NYC_Taxi_Analysis").getOrCreate()

# Parquet 파일 로드
df = spark.read.parquet("/opt/spark/yellow_tripdata_2024-01.parquet")
```

---

# 🔍 회고 (KPT)

## ✅ Keep

- 휴일에도 시간 내서 작업하기 !
  - 연휴 간 가족 행사가 많아서 바빴었는데 보통의 평일들 보다 더 잠을 줄여가며 하루 작업 시간을 유지하려고 했던 내 자신이 그래도 대견 !

## ⚠️ Problem

- 집에 있을 때도 팀원들과 더 적극적으로 소통하기 !
  - 미션을 진행할 때 조금 더 소통을 했으면 어떨까 싶다 ⇒ 그들의 시간도 존중하면서 서로 메세지를 남겨놓으면 나중에라도 확인할 수 있으니 더욱 많이 소통해야 정보도 많이 오갈 것..!

## 💡 Try

- 연휴에 안주했던 내 자신 다시 일상생활 패턴으로 돌리기 !!

---
