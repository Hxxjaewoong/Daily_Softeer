# Hyundai Softeer Wiki

# ğŸ“… 2025.01.30 ë¦¬ë·° & íšŒê³ 

---

# ğŸ“ ë¦¬ë·°

## Team Project

- ì„ íƒí•œ íŒ€ì˜ ëª¨ë“  íŒ€ì›ì˜ urlì„ ëª¨ì•„ì„œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ â‡’ **ì„ ìˆ˜ ì´ë¦„ê³¼ URLì„ ì‰½ê²Œ ë§¤í•‘í•˜ê³ , ë£¨í”„ë¥¼ í†µí•´ í¬ë¡¤ë§**
- ìƒê°ë³´ë‹¤ ë§¤ìš° ë²ˆê±°ëŸ¬ì› ë˜ ì‘ì—….. ë‚´ì¼ íŒ€ì›ë“¤ì„ ë§Œë‚˜ë©´ ë” ì‰½ê³  ìë™í™”í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ì§€ì— ëŒ€í•´ ë…¼ì˜í•´ë³¼ ì˜ˆì •

```python
players = {
    "Emiliano MartÃ­nez": {
        "premierleague": "https://www.premierleague.com/players/4245/Emiliano-MartÃ­nez/stats",
        "understat": "https://understat.com/player/4401"
    },
    "Robin Olsen": {
        "premierleague": "https://www.premierleague.com/players/11224/Robin-Olsen/stats",
        "understat": "https://understat.com/player/6962"
    },
    ....
```

## Mission

### ì–´ì œ í•´ê²°í•˜ì§€ ëª»í•˜ì˜€ë˜ pi ì‘ì—… ê²°ê³¼ë¬¼ì„ ì €ì¥í•˜ëŠ” ë° ì„±ê³µ

```python
from pyspark.sql import SparkSession
import os
import random

def inside(_):
    x, y = random.random(), random.random()
    return 1 if x*x + y*y < 1 else 0

if __name__ == "__main__":
    spark = SparkSession.builder.appName("Pi Estimation").getOrCreate()

    n = 1000000  # 100ë§Œ ê°œì˜ ìƒ˜í”Œ
    count = spark.sparkContext.parallelize(range(n)).map(inside).reduce(lambda a, b: a + b)
    pi_estimate = 4.0 * count / n

    output_dir = "/opt/spark/output"
    os.makedirs(output_dir, exist_ok=True)

    df = spark.createDataFrame([(pi_estimate,)], ["Estimated Pi Value"])

    #  CSV íŒŒì¼ë¡œ ì €ì¥
    csv_output_path = os.path.join(output_dir, "pi_estimate.csv")
    df.write.mode("overwrite").csv(csv_output_path, header=True)

    #  Parquet íŒŒì¼ë¡œ ì €ì¥
    parquet_output_path = os.path.join(output_dir, "pi_estimate.parquet")
    df.write.mode("overwrite").parquet(parquet_output_path)

    print(f"Estimated Pi Value: {pi_estimate}")
    print(f"CSV Result saved to {csv_output_path}")
    print(f"Parquet Result saved to {parquet_output_path}")

    spark.stop()
```

### Dockerë¥¼ í™œìš©í•œ Sparkë¥¼ Jupyter notebookì„ í†µí•´ ì‘ì—…í•´ë³´ê¸°

```yaml
command: >
  bash -c "pip install jupyter pandas matplotlib && 
  jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' & 
  /start-master.sh"
```

- í•´ë‹¹ ì½”ë“œë¥¼ docker composeì— ì¶”ê°€í•¨ìœ¼ë¡œì¨ ë¡œì»¬ì—ì„œ 8888í¬íŠ¸ì—ì„œ jupyter notebookì„ ì‹¤í–‰í–ˆë‹¤.
  - â‡’ ë§ˆìš´íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì‘ì—…ë¬¼ì„ ë‚¨ê¸°ë„ë¡ í•¨
- í•´ê²° ë° íŒ€ì›ê³¼ ë…¼ì˜ í•´ì•¼í• ì ..
  1. í•˜ë‚˜í•˜ë‚˜ ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë°›ì€ ë°ì´í„°ë¥¼ í™œìš©í•˜ê³  ìˆëŠ”ë°, ìë™ìœ¼ë¡œ ë‹¤ìš´ë°›ì„ ë°©ë²•ì´ ìˆì„ê¹Œ?
  2. Sparkë¼ëŠ” êµ‰ì¥í•œ íˆ´ì„ í™œìš©í•´ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ìˆëŠ” ê²ƒì¸ê°€?

```python
from pyspark.sql import SparkSession

# Spark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder.appName("NYC_Taxi_Analysis").getOrCreate()

# Parquet íŒŒì¼ ë¡œë“œ
df = spark.read.parquet("/opt/spark/yellow_tripdata_2024-01.parquet")
```

---

# ğŸ” íšŒê³  (KPT)

## âœ… Keep

- íœ´ì¼ì—ë„ ì‹œê°„ ë‚´ì„œ ì‘ì—…í•˜ê¸° !
  - ì—°íœ´ ê°„ ê°€ì¡± í–‰ì‚¬ê°€ ë§ì•„ì„œ ë°”ë¹´ì—ˆëŠ”ë° ë³´í†µì˜ í‰ì¼ë“¤ ë³´ë‹¤ ë” ì ì„ ì¤„ì—¬ê°€ë©° í•˜ë£¨ ì‘ì—… ì‹œê°„ì„ ìœ ì§€í•˜ë ¤ê³  í–ˆë˜ ë‚´ ìì‹ ì´ ê·¸ë˜ë„ ëŒ€ê²¬ !

## âš ï¸ Problem

- ì§‘ì— ìˆì„ ë•Œë„ íŒ€ì›ë“¤ê³¼ ë” ì ê·¹ì ìœ¼ë¡œ ì†Œí†µí•˜ê¸° !
  - ë¯¸ì…˜ì„ ì§„í–‰í•  ë•Œ ì¡°ê¸ˆ ë” ì†Œí†µì„ í–ˆìœ¼ë©´ ì–´ë–¨ê¹Œ ì‹¶ë‹¤ â‡’ ê·¸ë“¤ì˜ ì‹œê°„ë„ ì¡´ì¤‘í•˜ë©´ì„œ ì„œë¡œ ë©”ì„¸ì§€ë¥¼ ë‚¨ê²¨ë†“ìœ¼ë©´ ë‚˜ì¤‘ì—ë¼ë„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë”ìš± ë§ì´ ì†Œí†µí•´ì•¼ ì •ë³´ë„ ë§ì´ ì˜¤ê°ˆ ê²ƒ..!

## ğŸ’¡ Try

- ì—°íœ´ì— ì•ˆì£¼í–ˆë˜ ë‚´ ìì‹  ë‹¤ì‹œ ì¼ìƒìƒí™œ íŒ¨í„´ìœ¼ë¡œ ëŒë¦¬ê¸° !!

---
